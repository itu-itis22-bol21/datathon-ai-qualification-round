{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":90279,"databundleVersionId":10477255,"sourceType":"competition"}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nfrom torch import nn, optim\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\n# Define Custom Dataset\nclass CustomDataset(Dataset):\n    def __init__(self, csv_file, root_dir, transform=None, mode='train'):\n        self.data = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n        self.mode = mode\n        \n        if self.mode == 'train':\n            self.classes = sorted(self.data['city'].unique())\n            self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.root_dir, self.data.iloc[idx]['filename'])\n        image = Image.open(img_name).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        if self.mode == 'train':\n            city = self.data.iloc[idx]['city']\n            label = self.class_to_idx[city]\n            return image, label\n        else:\n            return image, self.data.iloc[idx]['filename']\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-16T05:31:13.878172Z","iopub.execute_input":"2024-12-16T05:31:13.878684Z","iopub.status.idle":"2024-12-16T05:31:20.486619Z","shell.execute_reply.started":"2024-12-16T05:31:13.878655Z","shell.execute_reply":"2024-12-16T05:31:20.485865Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"device = \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T05:31:20.488011Z","iopub.execute_input":"2024-12-16T05:31:20.488421Z","iopub.status.idle":"2024-12-16T05:31:20.576153Z","shell.execute_reply.started":"2024-12-16T05:31:20.488387Z","shell.execute_reply":"2024-12-16T05:31:20.575233Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"train_csv = '/kaggle/input/datathon-ai-qualification-round/train_data.csv'\ntest_csv = '/kaggle/input/datathon-ai-qualification-round/test.csv'\ntrain_dir = '/kaggle/input/datathon-ai-qualification-round/train/train'  # Adjust this to the correct path\ntest_dir = '/kaggle/input/datathon-ai-qualification-round/test/test'\n\n\n# Split training CSV into train and validation sets\ndf = pd.read_csv(train_csv)\ntrain_df, val_df = train_test_split(df, test_size=0.15, stratify=df['city'], random_state=42)\n\n# Save temporary CSVs for train and val to load with dataset class\ntrain_split_csv = \"train_split.csv\"\nval_split_csv = \"val_split.csv\"\ntrain_df.to_csv(train_split_csv, index=False)\nval_df.to_csv(val_split_csv, index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T05:31:20.577187Z","iopub.execute_input":"2024-12-16T05:31:20.577578Z","iopub.status.idle":"2024-12-16T05:31:20.627252Z","shell.execute_reply.started":"2024-12-16T05:31:20.577531Z","shell.execute_reply":"2024-12-16T05:31:20.626667Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"\n# Transform definitions\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\ntrain_transforms = transforms.Compose([\n    transforms.Resize((380, 380)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std),\n])\n\nval_transforms = transforms.Compose([\n    transforms.Resize((380, 380)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std)\n])\n\ntest_transforms = transforms.Compose([\n    transforms.Resize((380, 380)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std)\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T05:31:20.628700Z","iopub.execute_input":"2024-12-16T05:31:20.628945Z","iopub.status.idle":"2024-12-16T05:31:20.634650Z","shell.execute_reply.started":"2024-12-16T05:31:20.628922Z","shell.execute_reply":"2024-12-16T05:31:20.633772Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train_dataset = CustomDataset(csv_file=train_split_csv, root_dir=train_dir, transform=train_transforms, mode='train')\nval_dataset = CustomDataset(csv_file=val_split_csv, root_dir=train_dir, transform=val_transforms, mode='train')\ntest_dataset = CustomDataset(csv_file=test_csv, root_dir=test_dir, transform=test_transforms, mode='test')\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T05:31:20.635459Z","iopub.execute_input":"2024-12-16T05:31:20.635665Z","iopub.status.idle":"2024-12-16T05:31:20.662815Z","shell.execute_reply.started":"2024-12-16T05:31:20.635644Z","shell.execute_reply":"2024-12-16T05:31:20.662015Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nfrom torch import nn, optim\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnum_classes = len(train_dataset.class_to_idx)\n\n# Load the EfficientNet-B2 model with pretrained weights\nmodel = models.efficientnet_b2(weights=models.EfficientNet_B2_Weights.IMAGENET1K_V1)\n\n# Freeze all layers initially\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Unfreeze the last two blocks and classifier\nfor param in model.classifier.parameters():\n    param.requires_grad = True\nfor param in model.features[6:].parameters():  # Unfreeze last two blocks (6 and 7)\n    param.requires_grad = True\n\n# Modify the classification head\nmodel.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\nmodel = model.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Optimizer with different learning rates and weight decay\noptimizer = optim.Adam([\n    {'params': model.features[6:].parameters(), 'lr': 1e-5, 'weight_decay': 1e-5},  # Lower LR, weight decay for features\n    {'params': model.classifier.parameters(), 'lr': 1e-4, 'weight_decay': 1e-5}   # Higher LR, weight decay for classifier\n])\n\n# Learning Rate Scheduler (ReduceLROnPlateau)\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T05:31:20.663748Z","iopub.execute_input":"2024-12-16T05:31:20.663981Z","iopub.status.idle":"2024-12-16T05:31:21.410732Z","shell.execute_reply.started":"2024-12-16T05:31:20.663958Z","shell.execute_reply":"2024-12-16T05:31:21.409776Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/efficientnet_b2_rwightman-c35c1473.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b2_rwightman-c35c1473.pth\n100%|██████████| 35.2M/35.2M [00:00<00:00, 214MB/s]\n/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"epochs = 20 # Increased epochs\n\nbest_val_loss = float('inf') # For early stopping\npatience_counter = 0\n\nfor epoch in range(epochs):\n    # Training phase\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    for images, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{epochs}\"):\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * images.size(0)\n        _, preds = torch.max(outputs, 1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n    epoch_loss = running_loss / total\n    epoch_acc = correct / total\n\n    # Validation phase\n    model.eval()\n    val_running_loss = 0.0\n    val_correct = 0\n    val_total = 0\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            val_loss = criterion(outputs, labels)\n            val_running_loss += val_loss.item() * images.size(0)\n            _, val_preds = torch.max(outputs, 1)\n            val_correct += (val_preds == labels).sum().item()\n            val_total += labels.size(0)\n    val_epoch_loss = val_running_loss / val_total\n    val_epoch_acc = val_correct / val_total\n\n    scheduler.step(val_epoch_loss) # Step the LR scheduler\n\n    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc*100:.2f}% | \"\n          f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc*100:.2f}%\")\n    \n    # Early Stopping\n    if val_epoch_loss < best_val_loss:\n        best_val_loss = val_epoch_loss\n        torch.save(model.state_dict(), \"best_model.pth\") # Save the best model\n        patience_counter = 0\n    else:\n        patience_counter += 1\n        if patience_counter >= 5: # Stop if no improvement for 5 epochs\n            print(\"Early stopping triggered.\")\n            break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T05:31:21.411810Z","iopub.execute_input":"2024-12-16T05:31:21.412093Z","iopub.status.idle":"2024-12-16T05:31:47.574528Z","shell.execute_reply.started":"2024-12-16T05:31:21.412067Z","shell.execute_reply":"2024-12-16T05:31:47.573273Z"}},"outputs":[{"name":"stderr","text":"Training Epoch 1/20:  17%|█▋        | 32/186 [00:25<02:02,  1.26it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m---> 16\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:1188\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":7},{"cell_type":"code","source":"# Load best model\nmodel.load_state_dict(torch.load(\"best_model.pth\"))\n# ... (Your inference code)# Inference on Test\nmodel.eval()\npredictions = []\n\nidx_to_class = {v: k for k, v in train_dataset.class_to_idx.items()}\n\nwith torch.no_grad():\n    for images, file_names in tqdm(test_loader, desc=\"Predicting on Test Set\"):\n        images = images.to(device)\n        outputs = model(images)\n        _, preds = torch.max(outputs, 1)\n        \n        for f, p in zip(file_names, preds):\n            city_name = idx_to_class[p.item()]\n            predictions.append((f, city_name))\n\n# Save predictions\npred_df = pd.DataFrame(predictions, columns=[\"filename\", \"city\"])\npred_df.to_csv(\"test_predictionsefficiencynet32.csv\", index=False)\nprint(\"Predictions saved to test_predictionsefficiencynet32.csv\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T15:23:46.108377Z","iopub.status.idle":"2024-12-15T15:23:46.108851Z","shell.execute_reply.started":"2024-12-15T15:23:46.108609Z","shell.execute_reply":"2024-12-15T15:23:46.108634Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}